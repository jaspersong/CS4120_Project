{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "NUM_TRAINING_DATA = 529488\n",
    "NUM_TESTING_DATA = 125990\n",
    "NUM_AUTHORS = 13440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Functions\n",
    "\n",
    "Defines the functions used to stream in the vectors in the provided files for testing/training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def json_output_to_vector(json_output):\n",
    "    \"\"\"Takes in a loaded dictionary from the doc2vec output json and grabs the feature vector of that associated post.\"\"\"\n",
    "    split_list = json_output.strip('][').split(',')\n",
    "    ret_value = []\n",
    "    for item in split_list:\n",
    "        ret_value.append(float(item))\n",
    "    return ret_value\n",
    "\n",
    "seen_aids = set()  # Stores which author id we have seen thus far\n",
    "aid_to_id = {}  # Stores the mapping that converts the author id to an array index\n",
    "def aid_to_label(json_id):\n",
    "    \"\"\"Takes in a loaded dictionary from the doc2vec output json and grabs the label of that associated post. It then\n",
    "    outputs the label in the style of one-hot encoding in relation to the number of classes (number of authors)\"\"\"\n",
    "    global seen_aids\n",
    "    global aid_to_id\n",
    "    \n",
    "    aid = int(json_id)\n",
    "    label = np.zeros((NUM_AUTHORS))\n",
    "    \n",
    "    # Translate the aid to an array index\n",
    "    idx = -1\n",
    "    if aid in seen_aids:\n",
    "        idx = aid_to_id[aid]\n",
    "    else:\n",
    "        idx = len(seen_aids)\n",
    "        seen_aids.add(aid)\n",
    "        aid_to_id[aid] = idx\n",
    "    \n",
    "    # Set the label\n",
    "    label[idx] = 1\n",
    "    return label\n",
    "\n",
    "def corpus_generator(filepath):\n",
    "    \"\"\"An infinite generator that reads the provided file and creates batches of vector/label. \n",
    "    It automatically resets itself.\"\"\"\n",
    "    file = open(filepath, \"r\")\n",
    "    \n",
    "    while True:\n",
    "        batch_features = np.zeros((BATCH_SIZE, 50))\n",
    "        batch_labels = np.zeros((BATCH_SIZE, NUM_AUTHORS))\n",
    "        for i in range(BATCH_SIZE):\n",
    "            try:\n",
    "                data = json.loads(next(file))\n",
    "                batch_features[i] = json_output_to_vector(data['output'])\n",
    "                batch_labels[i] = aid_to_label(data['author_id'])\n",
    "            except StopIteration:\n",
    "                file.seek(0)\n",
    "                break\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model\n",
    "\n",
    "Defines the MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "\n",
    "def create_classifier():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_dim=50))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(NUM_AUTHORS, activation='relu'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Unigram Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_unigram_chars.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_unigram_chars.json\"\n",
    "\n",
    "unigram_chars = create_classifier()\n",
    "unigram_chars.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = unigram_chars.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Unigram Chars Accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Bigram Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_bigram_chars.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_bigram_chars.json\"\n",
    "\n",
    "bigram_chars = create_classifier()\n",
    "bigram_chars.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = bigram_chars.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Bigram Chars Accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Trigram Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_trigram_chars.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_trigram_chars.json\"\n",
    "\n",
    "trigram_chars = create_classifier()\n",
    "trigram_chars.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = trigram_chars.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Trigram Chars Accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Unigram Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_unigram_words.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_unigram_words.json\"\n",
    "\n",
    "unigram_words = create_classifier()\n",
    "unigram_words.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = unigram_words.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Unigram Words Accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Bigram Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_bigram_words.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_bigram_words.json\"\n",
    "\n",
    "bigram_words = create_classifier()\n",
    "bigram_words.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = bigram_words.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Bigram Words Accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Trigram Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_trigram_words.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_trigram_words.json\"\n",
    "\n",
    "trigram_words = create_classifier()\n",
    "trigram_words.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = trigram_words.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Trigram Words Accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Unigram POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_unigram_pos.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_unigram_pos.json\"\n",
    "\n",
    "unigram_pos = create_classifier()\n",
    "unigram_pos.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = unigram_pos.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Unigram POS Accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Bigram POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_bigram_pos.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_bigram_pos.json\"\n",
    "\n",
    "bigram_pos = create_classifier()\n",
    "bigram_pos.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = bigram_pos.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Bigram POS Accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Classifier: Trigram POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filepath = \"doc2vec_outputs/inferred_training_trigram_pos.json\"\n",
    "test_filepath = \"doc2vec_outputs/inferred_testing_trigram_pos.json\"\n",
    "\n",
    "trigram_pos = create_classifier()\n",
    "trigram_pos.fit_generator(corpus_generator(train_filepath), \n",
    "                          steps_per_epoch=math.ceil(NUM_TRAINING_DATA / BATCH_SIZE), \n",
    "                          epochs=10)\n",
    "result = trigram_pos.evaluate_generator(corpus_generator(test_filepath), \n",
    "                                        steps=math.ceil(NUM_TESTING_DATA / BATCH_SIZE))\n",
    "print(\"Trigram POS Accuracy:\", result[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
