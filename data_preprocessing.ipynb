{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Percentage of posts that will be pulled from each author's dataset as the testing set.\n",
    "PERCENTAGE_FOR_TESTING = 0.2\n",
    "\n",
    "# Minimum number of posts\n",
    "MIN_NUM_POSTS = 5\n",
    "\n",
    "# Minimum number of words\n",
    "MIN_NUM_WORDS = 1000\n",
    "\n",
    "# Determines whether or not a list of posts attributed to a single author\n",
    "# will be shuffled before being separated into training and testing datasets\n",
    "WILL_SHUFFLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Filename Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "blog_directory = \"blogs/\"\n",
    "json_directory = \"jsons/\"\n",
    "\n",
    "if not os.path.exists(json_directory):\n",
    "    os.makedirs(json_directory)\n",
    "\n",
    "unigram_chars_json_filename = \"unigram_chars.json\"\n",
    "bigram_chars_json_filename = \"bigram_chars.json\"\n",
    "trigram_chars_json_filename = \"trigram_chars.json\"\n",
    "unigram_words_json_filename = \"unigram_words.json\"\n",
    "bigram_words_json_filename = \"bigram_words.json\"\n",
    "trigram_words_json_filename = \"trigram_words.json\"\n",
    "unigram_pos_json_filename = \"unigram_pos.json\"\n",
    "bigram_pos_json_filename = \"bigram_pos.json\"\n",
    "trigram_pos_json_filename = \"trigram_pos.json\"\n",
    "\n",
    "leftover_json_filename = \"leftover.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Builders\n",
    "\n",
    "Classes that will take characters, words, POS tags and builds a list of n-grams. Each builder can be re-used after calling package_and_reset()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramBuilder:\n",
    "    def __init__(self, n):\n",
    "        self.__n = n\n",
    "        self.__all_ngrams = []  # Holds the list of all of the n-grams\n",
    "        self.__ngram = []  # Temporary list holding the n-gram that is being built\n",
    "\n",
    "    def add(self, item):\n",
    "        \"\"\"Takes in an item and adds it to the n-grams\"\"\"\n",
    "        self.__ngram.append(item)\n",
    "        if len(self.__ngram) == self.__n:\n",
    "            # Completed a whole character n-gram. Add it to the list\n",
    "            self.__all_ngrams.append(tuple(self.__ngram))\n",
    "            self.__ngram.pop(0)  # Make room for the next character\n",
    "\n",
    "    def package_and_reset(self):\n",
    "        \"\"\"Return the fully built n-gram list and then reset the builder for reuse.\"\"\"\n",
    "        ret_value = self.__all_ngrams\n",
    "        self.__all_ngrams = []\n",
    "        self.__ngram = []\n",
    "\n",
    "        return ret_value\n",
    "\n",
    "\n",
    "class CharacterNGramBuilder(NGramBuilder):\n",
    "    def add_char(self, char: str):\n",
    "        \"\"\"Takes in a character and adds it to the character n-grams\"\"\"\n",
    "        self.add(char)\n",
    "\n",
    "\n",
    "class WordNGramBuilder(NGramBuilder):\n",
    "    def add_word(self, token):\n",
    "        \"\"\"Takes in a spacy token and adds it to the word n-grams.\"\"\"\n",
    "        self.add(token.text)\n",
    "\n",
    "\n",
    "class POSNGramBuilder(NGramBuilder):\n",
    "    def add_postag(self, token):\n",
    "        \"\"\"Takes in a spacy token and adds it to the POS n-grams\"\"\"\n",
    "        self.add(token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to Disk: JSON Storage\n",
    "\n",
    "Due to the large amount of data, objects will be serialized into JSON objects and dumped into `*.json` files in order to free up memory during the preprocessing phase. This results in multiple JSON objects added to the end of the json file specified. These values can be loaded in at another time using `json.load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JSONStorage:\n",
    "    \"\"\"Dumps items into a json file\"\"\"\n",
    "    def __init__(self, file_name: str):\n",
    "        self.__file = open(file_name, \"a\")\n",
    "\n",
    "    def close(self):\n",
    "        self.__file.close()\n",
    "\n",
    "    def add(self, item: dict):\n",
    "        self.__file.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Posts\n",
    "\n",
    "This defines a function that takes in the filename of an author's \n",
    "dataset file, reads the data, and returns the id number of the author\n",
    "and a list of the author's posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_posts(data_directory: str, data_filename: str):\n",
    "    # Get the author id using regex\n",
    "    a_id = re.search(\"^(\\d+)\", data_filename).group(1)\n",
    "    \n",
    "    # Get the array of the author's posts\n",
    "    post_list = []\n",
    "    file = open(data_directory + data_filename, \"r\", encoding='latin1')\n",
    "    post_mode = False\n",
    "    post = \"\"\n",
    "    for line in file.readlines():\n",
    "        line = line.strip()\n",
    "        if re.search(\"^<post>$\", line):\n",
    "            # Found the start of a post\n",
    "            post_mode = True\n",
    "        elif re.search(\"^</post>$\", line):\n",
    "            # End of the post. Clean up the post and add it to the list\n",
    "            post_mode = False\n",
    "            post = post.strip()\n",
    "            post_list.append(post)\n",
    "            post = \"\"\n",
    "        elif post_mode:\n",
    "            # Concatenate the line to the post\n",
    "            post += line\n",
    "    \n",
    "    return a_id, post_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data\n",
    "\n",
    "Takes in a list of posts and creates a list of dictionaries with the following information:\n",
    "- unigram_chars: A list of the unigram of each character in order of the sentence\n",
    "- bigram_chars: A list of the bigram of each character in order of the sentence\n",
    "- trigram_chars: A list of the trigram of each character in order of the sentence\n",
    "- unigram_words: A list of the unigrams in order of the sentence\n",
    "- bigram_words: A list of the bigrams in order of the sentence\n",
    "- trigram_words: A list of the trigrams in order of the sentence\n",
    "- unigram_pos: A list of the unigram of each POS tag in order of the sentence\n",
    "- bigram_pos: A list of the bigram of each POS tag in order of the sentence\n",
    "- trigram_pos: A list of the trigram of each POS tag in order of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def tokenize_data(post_list: list):\n",
    "    global all_word_frequency\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    ret_value = []\n",
    "    unigram_chars_builder = CharacterNGramBuilder(1)\n",
    "    bigram_chars_builder = CharacterNGramBuilder(2)\n",
    "    trigram_chars_builder = CharacterNGramBuilder(3)\n",
    "    unigram_words_builder = WordNGramBuilder(1)\n",
    "    bigram_words_builder = WordNGramBuilder(2)\n",
    "    trigram_words_builder = WordNGramBuilder(3)\n",
    "    unigram_pos_builder = POSNGramBuilder(1)\n",
    "    bigram_pos_builder = POSNGramBuilder(2)\n",
    "    trigram_pos_builder = POSNGramBuilder(3)\n",
    "\n",
    "    # Iterate through each post\n",
    "    for post in post_list:\n",
    "        post_doc = nlp(post)\n",
    "\n",
    "        # Handle at a cahracter level\n",
    "        for char in post:\n",
    "            unigram_chars_builder.add_char(char)\n",
    "            bigram_chars_builder.add_char(char)\n",
    "            trigram_chars_builder.add_char(char)\n",
    "\n",
    "        # Now handle at a token level\n",
    "        for token in post_doc:\n",
    "            word = token.text\n",
    "\n",
    "            # Add the word to the global word frequency of the whole corpus\n",
    "            if word in all_word_frequency.keys():\n",
    "                all_word_frequency[word] += 1\n",
    "            else:\n",
    "                all_word_frequency[word] = 1\n",
    "\n",
    "            unigram_words_builder.add_word(token)\n",
    "            bigram_words_builder.add_word(token)\n",
    "            trigram_words_builder.add_word(token)\n",
    "            unigram_pos_builder.add_postag(token)\n",
    "            bigram_pos_builder.add_postag(token)\n",
    "            trigram_pos_builder.add_postag(token)\n",
    "\n",
    "        # Now add the information into a dictionary and add it to the return value list\n",
    "        post_info = {\"unigram_chars\": unigram_chars_builder.package_and_reset(),\n",
    "                     \"bigram_chars\": bigram_chars_builder.package_and_reset(),\n",
    "                     \"trigram_chars\": trigram_chars_builder.package_and_reset(),\n",
    "                     \"unigram_words\": unigram_words_builder.package_and_reset(),\n",
    "                     \"bigram_words\": bigram_words_builder.package_and_reset(),\n",
    "                     \"trigram_words\": trigram_words_builder.package_and_reset(),\n",
    "                     \"unigram_pos\": unigram_pos_builder.package_and_reset(),\n",
    "                     \"bigram_pos\": bigram_pos_builder.package_and_reset(),\n",
    "                     \"trigram_pos\": trigram_pos_builder.package_and_reset()}\n",
    "        ret_value.append(post_info)\n",
    "\n",
    "    return ret_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Separate Data\n",
    "\n",
    "Separates the list of posts put through tokenize_data() into the training dataset and the testing dataset by the percentage outlined by PERCENTAGE_FOR_TESTING. It assumes that all of the posts within the list is attributed to the author id provided. If the post list does not reach the minimum word count or the minimum post count, then it will delegate the data into the leftover_data set. \n",
    "\n",
    "The data will be loaded into json files for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def add_data_to_leftover(a_id: int, post_list: list):\n",
    "    global leftover_json\n",
    "    for post_id in range(len(post_list)):\n",
    "        post_info = {\"author_id\": a_id, \"post_id\": post_id, \"post_info\": post_list[post_id]}\n",
    "        leftover_json.add(post_info)\n",
    "\n",
    "\n",
    "def add_data_to_train(a_id: int, post_list: list):\n",
    "    global train_unigram_chars_json\n",
    "    global train_bigram_chars_json\n",
    "    global train_trigram_chars_json\n",
    "    global train_unigram_words_json\n",
    "    global train_bigram_words_json\n",
    "    global train_trigram_words_json\n",
    "    global train_unigram_pos_json\n",
    "    global train_bigram_pos_json\n",
    "    global train_trigram_pos_json\n",
    "\n",
    "    for post_id in range(len(post_list)):\n",
    "        post_info = post_list[post_id]\n",
    "        train_unigram_chars_json.add({\"author_id\": a_id, \"post_id\": post_id, \"unigram_chars\": post_info['unigram_chars']})\n",
    "        train_bigram_chars_json.add({\"author_id\": a_id, \"post_id\": post_id, \"bigram_chars\": post_info['bigram_chars']})\n",
    "        train_trigram_chars_json.add({\"author_id\": a_id, \"post_id\": post_id, \"trigram_chars\": post_info['trigram_chars']})\n",
    "        train_unigram_words_json.add({\"author_id\": a_id, \"post_id\": post_id, \"unigram_words\": post_info['unigram_words']})\n",
    "        train_bigram_words_json.add({\"author_id\": a_id, \"post_id\": post_id, \"bigram_words\": post_info['bigram_words']})\n",
    "        train_trigram_words_json.add({\"author_id\": a_id, \"post_id\": post_id, \"trigram_words\": post_info['trigram_words']})\n",
    "        train_unigram_pos_json.add({\"author_id\": a_id, \"post_id\": post_id, \"unigram_pos\": post_info['unigram_pos']})\n",
    "        train_bigram_pos_json.add({\"author_id\": a_id, \"post_id\": post_id, \"bigram_pos\": post_info['bigram_pos']})\n",
    "        train_trigram_pos_json.add({\"author_id\": a_id, \"post_id\": post_id, \"trigram_pos\": post_info['trigram_pos']})\n",
    "\n",
    "\n",
    "def add_data_to_test(a_id: int, post_list: list):\n",
    "    global test_unigram_chars_json\n",
    "    global test_bigram_chars_json\n",
    "    global test_trigram_chars_json\n",
    "    global test_unigram_words_json\n",
    "    global test_bigram_words_json\n",
    "    global test_trigram_words_json\n",
    "    global test_unigram_pos_json\n",
    "    global test_bigram_pos_json\n",
    "    global test_trigram_pos_json\n",
    "\n",
    "    for post_id in range(len(post_list)):\n",
    "        post_info = post_list[post_id]\n",
    "        test_unigram_chars_json.add({\"author_id\": a_id, \"post_id\": post_id, \"unigram_chars\": post_info['unigram_chars']})\n",
    "        test_bigram_chars_json.add({\"author_id\": a_id, \"post_id\": post_id, \"bigram_chars\": post_info['bigram_chars']})\n",
    "        test_trigram_chars_json.add({\"author_id\": a_id, \"post_id\": post_id, \"trigram_chars\": post_info['trigram_chars']})\n",
    "        test_unigram_words_json.add({\"author_id\": a_id, \"post_id\": post_id, \"unigram_words\": post_info['unigram_words']})\n",
    "        test_bigram_words_json.add({\"author_id\": a_id, \"post_id\": post_id, \"bigram_words\": post_info['bigram_words']})\n",
    "        test_trigram_words_json.add({\"author_id\": a_id, \"post_id\": post_id, \"trigram_words\": post_info['trigram_words']})\n",
    "        test_unigram_pos_json.add({\"author_id\": a_id, \"post_id\": post_id, \"unigram_pos\": post_info['unigram_pos']})\n",
    "        test_bigram_pos_json.add({\"author_id\": a_id, \"post_id\": post_id, \"bigram_pos\": post_info['bigram_pos']})\n",
    "        test_trigram_pos_json.add({\"author_id\": a_id, \"post_id\": post_id, \"trigram_pos\": post_info['trigram_pos']})\n",
    "\n",
    "\n",
    "def separate_data(a_id: int, post_list: list):\n",
    "    global MIN_NUM_POSTS\n",
    "    global MIN_NUM_WORDS\n",
    "    global WILL_SHUFFLE\n",
    "    global PERCENTAGE_FOR_TESTING\n",
    "\n",
    "    if WILL_SHUFFLE:\n",
    "        random.shuffle(post_list)\n",
    "\n",
    "    # Ensure that the posts meet the minimum requirements\n",
    "    num_posts = len(post_list)\n",
    "    num_words = 0\n",
    "    for post in post_list:\n",
    "        num_words += len(post[\"unigram_words\"])\n",
    "\n",
    "    if num_posts >= MIN_NUM_POSTS and num_words >= MIN_NUM_WORDS:\n",
    "        # Separate the post list into two parts, where one is PERCENTAGE_FOR_TESTING\n",
    "        # of the whole post list\n",
    "        num_posts_testing = math.floor(len(post_list) * PERCENTAGE_FOR_TESTING)\n",
    "        testing_set = post_list[:num_posts_testing]\n",
    "        training_set = post_list[num_posts_testing:]\n",
    "\n",
    "        # Now add them to the respective global dataset\n",
    "        add_data_to_test(a_id, testing_set)\n",
    "        add_data_to_train(a_id, training_set)\n",
    "    else:\n",
    "        # This post list doesn't make the cut. Put it in the leftover set\n",
    "        add_data_to_leftover(a_id, post_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_word_frequency = {}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open JSON Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram_chars_json = JSONStorage(json_directory + \"train_\" + unigram_chars_json_filename)\n",
    "train_bigram_chars_json = JSONStorage(json_directory + \"train_\" + bigram_chars_json_filename)\n",
    "train_trigram_chars_json = JSONStorage(json_directory + \"train_\" + trigram_chars_json_filename)\n",
    "train_unigram_words_json = JSONStorage(json_directory + \"train_\" + unigram_words_json_filename)\n",
    "train_bigram_words_json = JSONStorage(json_directory + \"train_\" + bigram_words_json_filename)\n",
    "train_trigram_words_json = JSONStorage(json_directory + \"train_\" + trigram_words_json_filename)\n",
    "train_unigram_pos_json = JSONStorage(json_directory + \"train_\" + unigram_pos_json_filename)\n",
    "train_bigram_pos_json = JSONStorage(json_directory + \"train_\" + bigram_pos_json_filename)\n",
    "train_trigram_pos_json = JSONStorage(json_directory + \"train_\" + trigram_pos_json_filename)\n",
    "\n",
    "test_unigram_chars_json = JSONStorage(json_directory + \"test_\" + unigram_chars_json_filename)\n",
    "test_bigram_chars_json = JSONStorage(json_directory + \"test_\" + bigram_chars_json_filename)\n",
    "test_trigram_chars_json = JSONStorage(json_directory + \"test_\" + trigram_chars_json_filename)\n",
    "test_unigram_words_json = JSONStorage(json_directory + \"test_\" + unigram_words_json_filename)\n",
    "test_bigram_words_json = JSONStorage(json_directory + \"test_\" + bigram_words_json_filename)\n",
    "test_trigram_words_json = JSONStorage(json_directory + \"test_\" + trigram_words_json_filename)\n",
    "test_unigram_pos_json = JSONStorage(json_directory + \"test_\" + unigram_pos_json_filename)\n",
    "test_bigram_pos_json = JSONStorage(json_directory + \"test_\" + bigram_pos_json_filename)\n",
    "test_trigram_pos_json = JSONStorage(json_directory + \"test_\" + trigram_pos_json_filename)\n",
    "\n",
    "leftover_json = JSONStorage(json_directory + leftover_json_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all of the files\n",
    "print(\"Reading through data...\")\n",
    "num_files = len(os.listdir(blog_directory))\n",
    "i = 1\n",
    "for filename in os.listdir(blog_directory):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        author_id, posts = get_posts(blog_directory, filename)\n",
    "        posts = tokenize_data(posts)\n",
    "        separate_data(author_id, posts)\n",
    "        print(i, \"/\", num_files, \": Finished processing author\", author_id)\n",
    "        i += 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outputting vacabulary data to json file...\")\n",
    "\n",
    "train_unigram_chars_json.close()\n",
    "train_bigram_chars_json.close()\n",
    "train_trigram_chars_json.close()\n",
    "train_unigram_words_json.close()\n",
    "train_bigram_words_json.close()\n",
    "train_trigram_words_json.close()\n",
    "train_unigram_pos_json.close()\n",
    "train_bigram_pos_json.close()\n",
    "train_trigram_pos_json.close()\n",
    "\n",
    "test_unigram_chars_json.close()\n",
    "test_bigram_chars_json.close()\n",
    "test_trigram_chars_json.close()\n",
    "test_unigram_words_json.close()\n",
    "test_bigram_words_json.close()\n",
    "test_trigram_words_json.close()\n",
    "test_unigram_pos_json.close()\n",
    "test_bigram_pos_json.close()\n",
    "test_trigram_pos_json.close()\n",
    "\n",
    "leftover_json.close()\n",
    "\n",
    "with open(json_directory + 'vocabulary.json', 'w') as json_file:\n",
    "    json.dump(all_word_frequency, json_file)\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
